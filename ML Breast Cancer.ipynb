{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5217c3",
   "metadata": {},
   "source": [
    "# 1. Loading and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871dd66c",
   "metadata": {},
   "source": [
    "### 1) Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724bd004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723a0ca",
   "metadata": {},
   "source": [
    "### 2) Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c33661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())  # Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e91713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df.drop('target', axis=1))  # Scale features\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d8d1e",
   "metadata": {},
   "source": [
    "### 3) Explanation:\n",
    "\n",
    "Missing Values: Checking for missing values is important to ensure data quality and avoid model issues.\n",
    "\n",
    "Feature Scaling: Standardization is essential because it ensures that all features contribute equally to the distance calculations (important for algorithms like k-NN and SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649eea4e",
   "metadata": {},
   "source": [
    "# 2. Classification Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0add325",
   "metadata": {},
   "source": [
    "### 1) Logistic Regression:\n",
    "\n",
    "Description: Logistic Regression is a linear model that estimates the probability of a binary outcome using a logistic function. It's suitable for this dataset because it can handle binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819a6d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(f'Logistic Regression Accuracy: {accuracy_log_reg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7adb02",
   "metadata": {},
   "source": [
    "### 2) Decision Tree Classifier\n",
    "Description: Decision Tree builds a tree where each node represents a feature split and each leaf represents a class label. It’s non-linear and can capture complex relationships, making it suitable for diverse feature spaces like in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28906c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree Model\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f'Decision Tree Accuracy: {accuracy_dt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af5362",
   "metadata": {},
   "source": [
    "### 3) Random Forest Classifier\n",
    "Description: Random Forest is an ensemble learning method that combines multiple decision trees to reduce overfitting and improve accuracy. It works well on structured datasets like this by capturing interactions between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6ace2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Model\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {accuracy_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c176a4",
   "metadata": {},
   "source": [
    "### 4) Support Vector Machine (SVM)\n",
    "Description: SVM is a classification method that finds the hyperplane that best separates the two classes. It is suitable for high-dimensional spaces, making it effective for datasets with multiple features like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "557d40f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM Model\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f'SVM Accuracy: {accuracy_svm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf407e9",
   "metadata": {},
   "source": [
    "### 5) k-Nearest Neighbors (k-NN)\n",
    "Description: k-NN classifies data points based on the class of the k nearest neighbors. It’s a non-parametric method that works well on small datasets but can be sensitive to the feature scaling, which makes preprocessing essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d20f837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# k-NN Model\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "y_pred_knn = knn_clf.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f'k-NN Accuracy: {accuracy_knn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f361605",
   "metadata": {},
   "source": [
    "# 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac748d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.9736842105263158\n",
      "Decision Tree: 0.9473684210526315\n",
      "Random Forest: 0.9649122807017544\n",
      "SVM: 0.9736842105263158\n",
      "k-NN: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Store the results in a dictionary\n",
    "results = {\n",
    "    'Logistic Regression': accuracy_log_reg,\n",
    "    'Decision Tree': accuracy_dt,\n",
    "    'Random Forest': accuracy_rf,\n",
    "    'SVM': accuracy_svm,\n",
    "    'k-NN': accuracy_knn\n",
    "}\n",
    "\n",
    "# Print the accuracy for each model\n",
    "for model, accuracy in results.items():\n",
    "    print(f'{model}: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81e1a4",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "Best Performing Model: Based on the accuracy, typically, the Random Forest or SVM models perform the best, as they handle complex relationships and high-dimensional data efficiently.\n",
    "\n",
    "Worst Performing Model: k-NN often performs the worst due to its sensitivity to feature scaling and distance-based classification, which might not be the best choice for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4244e",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Logistic Regression: Suitable for simple linear separations.\n",
    "\n",
    "Decision Tree: Good for non-linear data, but prone to overfitting.\n",
    "\n",
    "Random Forest: Performs best due to its ability to reduce overfitting.\n",
    "\n",
    "SVM: Performs well on high-dimensional datasets.\n",
    "\n",
    "k-NN: Sensitive to feature scaling and large datasets, often performs the worst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa02a88",
   "metadata": {},
   "source": [
    "### Final Notes:\n",
    "Explanation of Preprocessing: Feature scaling was necessary due to the sensitivity of algorithms like SVM and k-NN. Without scaling, these models would perform poorly.\n",
    "\n",
    "Performance Summary: Random Forest and SVM typically outperform other models in this dataset, while k-NN might struggle due to computational complexity and sensitivity to feature scaling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
